{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\Desktop\\Semester\\BCI\\create_dataset.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"train_{}/labels.npy\".format(experiment), np.hstack(np.array(train_labels)))\n",
      "c:\\Users\\Daydreamore\\Desktop\\Semester\\BCI\\create_dataset.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"val_{}/labels.npy\".format(experiment), np.hstack(np.array(val_labels)))\n",
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Helper functions\n",
    "from create_dataset import make_dataset\n",
    "\n",
    "#Deep Learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import dataset as ds\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "#Logging\n",
    "#!wandb login 6b63fbb174d08e296b363d52818553c19d89e43d\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a dataset from the sub_preprocessed.npy files, where the samples for each condition are in separate folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\Desktop\\Semester\\BCI\\create_dataset.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"train_{}/labels.npy\".format(experiment), np.hstack(np.array(train_labels)))\n",
      "c:\\Users\\Daydreamore\\Desktop\\Semester\\BCI\\create_dataset.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"val_{}/labels.npy\".format(experiment), np.hstack(np.array(val_labels)))\n"
     ]
    }
   ],
   "source": [
    "#Create dataset\n",
    "make_dataset(\"C:/Users/Daydreamore/Desktop/Semester/BCI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Dataset-Class that can be indexed by Pytorch DataLoader.\n",
    "To save some GPU resources, we only pass the path such that the DataLoader (generator) loads one by one\n",
    "\n",
    "Note: use \"eeg_dataset\" to load all data or \"eeg_dataset_undersample\" to have a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class without undersampling\n",
    "class eeg_dataset():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.sample_list = os.listdir(path)[:-1]\n",
    "        self.targets = torch.from_numpy(np.load(os.path.join(path,os.listdir(path)[-1])))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (eeg_data, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        sample = np.load(os.path.join(self.path, self.sample_list[idx]))\n",
    "        # mean = np.mean(sample)\n",
    "        # std = np.std(sample)\n",
    "        # sample = (sample-mean)/std\n",
    "        return torch.from_numpy(sample), nn.functional.one_hot(self.targets[idx].to(torch.int64), num_classes = 3).float()\n",
    "\n",
    "path_train_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_visual\"\n",
    "path_train_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_multi\"\n",
    "path_val_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_visual\"\n",
    "path_val_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_multi\"\n",
    "train_set_visual = eeg_dataset(path_train_visual)\n",
    "train_set_multi = eeg_dataset(path_train_multi)\n",
    "val_set_visual = eeg_dataset(path_val_visual)\n",
    "val_set_multi = eeg_dataset(path_val_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1544])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_visual\"\n",
    "targets = torch.from_numpy(np.load(os.path.join(path,os.listdir(path)[-1])))\n",
    "torch.ones_like(targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class with undersampling\n",
    "class eeg_dataset_undersample():\n",
    "\n",
    "    def __init__(self, path, classes):\n",
    "        self.path = path\n",
    "        self.sample_list = []\n",
    "        self.target_list = []\n",
    "\n",
    "        samples = np.array(os.listdir(path)[:-1])\n",
    "        targets = torch.from_numpy(np.load(os.path.join(path,os.listdir(path)[-1])))\n",
    "        n_samp = np.where(targets == classes[1])[0].shape[0] #number of samples for condition (used to sample same number from control)\n",
    "        #for i in targets.unique(): #iterate over conditions (control, explosion, burning)\n",
    "        for i in classes:\n",
    "            target_ixs = np.where(targets == i)\n",
    "            if i == 0: \n",
    "                target_ixs_shuffled = np.random.choice(target_ixs[0], size = n_samp, replace = False) #randomly sample n_samp control to get balanced dataset\n",
    "                self.sample_list.append(samples[target_ixs_shuffled])\n",
    "                self.target_list.append(targets[target_ixs_shuffled])\n",
    "\n",
    "            else:\n",
    "                self.sample_list.append(samples[target_ixs]) \n",
    "                self.target_list.append(torch.ones_like(targets[target_ixs]))\n",
    "\n",
    "        self.sample_list = np.concatenate(self.sample_list, axis=0)\n",
    "        self.target_list = torch.cat(self.target_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (eeg_data, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        sample = np.load(os.path.join(self.path, self.sample_list[idx]))\n",
    "        # mean = np.mean(sample)\n",
    "        # std = np.std(sample)\n",
    "        # sample = (sample-mean)/std\n",
    "        return torch.from_numpy(sample), nn.functional.one_hot(self.target_list[idx].to(torch.int64), num_classes = 2).float()\n",
    "\n",
    "path_train_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_visual\"\n",
    "path_train_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_multi\"\n",
    "path_val_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_visual\"\n",
    "path_val_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_multi\"\n",
    "train_set_visual_u_01 = eeg_dataset_undersample(path_train_visual, classes = [0,1])\n",
    "train_set_multi_u_01 = eeg_dataset_undersample(path_train_multi, classes = [0,1])\n",
    "val_set_visual_u_01 = eeg_dataset_undersample(path_val_visual, classes = [0,1])\n",
    "val_set_multi_u_01 = eeg_dataset_undersample(path_val_multi, classes = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_visual_u_02 = eeg_dataset_undersample(path_train_visual, classes = [0,2])\n",
    "train_set_multi_u_02 = eeg_dataset_undersample(path_train_multi, classes = [0,2])\n",
    "val_set_visual_u_02 = eeg_dataset_undersample(path_val_visual, classes = [0,2])\n",
    "val_set_multi_u_02 = eeg_dataset_undersample(path_val_multi, classes = [0,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if values in the sample can be accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8946, dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_visual_u_01.__getitem__(1)[0][0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a highly unblanaced dataset. Therefore, we calculate a weight parameter to give a higher penalty to missprediction of less frequent class occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.from_numpy(train_set_visual_u_01.__len__() / (2 * np.bincount(train_set_visual_u_01.target_list))).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 1D-ConvNet Setup (this could be further modularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_set = train_set_visual_u_02,\n",
    "        val_set = val_set_visual_u_02,\n",
    "        batch_size = 16,\n",
    "        epochs = 100,\n",
    "        learning_rate = 0.00004,\n",
    "        in_channels = 11,\n",
    "        out_channels = 256,\n",
    "        kernel_size = 5,\n",
    "        num_classes = 2,\n",
    "        bn_alpha = 0.1,\n",
    "        pool_out1 = 120,\n",
    "        pool_out2 = 60,\n",
    "        pool_out3 = 30,\n",
    "        pool_out4 = 15,\n",
    "        class_weights = class_weights,\n",
    "        dropout = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #Model Architecture Stuff\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size) \n",
    "        self.conv3 = nn.Conv1d(out_channels, 128, kernel_size) \n",
    "        self.conv4 = nn.Conv1d(128, 64, kernel_size)\n",
    "\n",
    "        #self.pool = nn.MaxPool1d(kernel_size=pool_kernel) #stride = kernel_size\n",
    "        self.pool1 = nn.AdaptiveMaxPool1d(pool_out1)\n",
    "        self.pool2 = nn.AdaptiveMaxPool1d(pool_out2)\n",
    "        self.pool3 = nn.AdaptiveMaxPool1d(pool_out3)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(pool_out4)\n",
    "        self.pool_final = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lazy_linear = nn.LazyLinear(out_features = num_classes)\n",
    "        self.lazy_bn = nn.LazyBatchNorm1d()\n",
    "        self.lazy_bn2 = nn.LazyBatchNorm1d()\n",
    "        self.lazy_bn3 = nn.LazyBatchNorm1d()\n",
    "        self.GELU = nn.GELU()\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.initialize_weights()\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        #self.dataset = dataset\n",
    "        #self.train_sampler = train_sampler\n",
    "        self.stepsize = np.around(self.train_set.__len__()*0.8/self.batch_size) #for cycling lr\n",
    "        #self.val_sampler = val_sampler\n",
    "        self.class_weights = class_weights #torch.from_numpy(train_set.__len__() / (2 * np.bincount(train_set.targets)))\n",
    "        self.loss = nn.CrossEntropyLoss(weight = self.class_weights) \n",
    "        self.acc = Accuracy(task = \"multiclass\", num_classes = num_classes)\n",
    "\n",
    "        #Log Hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #[32, 256, 166]\n",
    "        x = self.lazy_bn(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool1(x) #[32, 256, 120]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x) #[32, 256, 114]\n",
    "        x = self.lazy_bn(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool2(x) #[32, 256, 60]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x) #[32, 128, 56]\n",
    "        x = self.lazy_bn2(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool3(x) #[32, 128, 30]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x) #[32, 64, 26]\n",
    "        x = self.lazy_bn3(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool4(x) #[32, 64, 15]\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool_final(x) #[32, 64, 1]\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.lazy_linear(x)\n",
    "        x = self.GELU(x) #[32 x 3]\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(params = self.parameters(), lr = self.learning_rate, weight_decay = 0.01) #wd = 0.01 by default #lr_before = 0.00043\n",
    "        #parameters for the cycling lr scheduler are chosen according to Smith (2015): https://arxiv.org/pdf/1506.01186.pdf\n",
    "        lr = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer, base_lr = self.learning_rate,\n",
    "            max_lr = 4*self.learning_rate,\n",
    "            step_size_up = 4*int(self.stepsize),\n",
    "            mode = \"triangular\",\n",
    "            cycle_momentum = False\n",
    "            )\n",
    "        #Fix pickling bug for cycling learning rate (https://github.com/pytorch/pytorch/issues/88684)\n",
    "        #instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        lr._scale_fn_custom = lr._scale_fn_ref()\n",
    "        #remove the reference so there are no more WeakMethod references in the object\n",
    "        lr._scale_fn_ref = None\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": lr,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"Learning Rate Scheduling\"\n",
    "        }\n",
    "        # return {\"optimizer\": optimizer,\n",
    "        #         \"lr_scheduler\": lr_scheduler}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logit = self.forward(x.float())\n",
    "        train_loss = self.loss(logit, y)\n",
    "        _, y_pred = torch.max(logit, dim = 1)\n",
    "        _, y_true = torch.max(y, dim = 1)\n",
    "        return {\"loss\": train_loss, \"y_pred\": y_pred, \"y_true\": y_true}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss_epoch = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        y_pred_epoch = torch.stack([x[\"y_pred\"] for x in outputs]).flatten()\n",
    "        y_true_epoch = torch.stack([x[\"y_true\"] for x in outputs]).flatten()\n",
    "        class_acc = torch.bincount(y_true_epoch[y_true_epoch == y_pred_epoch])\n",
    "        class_count = torch.bincount(y_true_epoch)\n",
    "\n",
    "        #Log rel. amount of falsely predicted targets per class\n",
    "        for ix, tclass in enumerate(class_acc):\n",
    "            if tclass == 0:\n",
    "                self.log(f\"class{ix}_acc_train\", 0.0, on_epoch = True, prog_bar = False)\n",
    "            else:\n",
    "                self.log(f\"class{ix}_acc_train\", tclass/class_count[ix], on_epoch = True, prog_bar = False)\n",
    "        \n",
    "        train_acc = self.acc(y_pred_epoch, y_true_epoch)\n",
    "        self.log(\"train/loss\", train_loss_epoch, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"train/acc\", train_acc, on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logit = self.forward(x.float())\n",
    "        val_loss = self.loss(logit, y)\n",
    "        _, y_pred = torch.max(logit, dim = 1)\n",
    "        _, y_true = torch.max(y, dim = 1)\n",
    "        return {\"loss\": val_loss, \"y_pred\": y_pred, \"y_true\": y_true}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss_epoch = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        y_pred_epoch = torch.stack([x[\"y_pred\"] for x in outputs]).flatten()\n",
    "        y_true_epoch = torch.stack([x[\"y_true\"] for x in outputs]).flatten()\n",
    "        class_acc = torch.bincount(y_true_epoch[y_true_epoch == y_pred_epoch])\n",
    "        class_count = torch.bincount(y_true_epoch)\n",
    "\n",
    "        #Log rel. amount of falsely predicted targets per class\n",
    "        for ix, tclass in enumerate(class_acc):\n",
    "            if tclass == 0:\n",
    "                self.log(f\"class{ix}_acc_val\", 0.0, on_epoch = True, prog_bar = False)\n",
    "            else:\n",
    "                self.log(f\"class{ix}_acc_val\", tclass/class_count[ix], on_epoch = True, prog_bar = False)\n",
    "                \n",
    "        val_acc = self.acc(y_pred_epoch, y_true_epoch)\n",
    "        self.log(\"val/loss\", val_loss_epoch, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"val/acc\", val_acc, on_epoch = True, prog_bar = True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(self.train_set, batch_size = self.batch_size,\n",
    "                                           shuffle = True, drop_last = True)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(self.val_set, batch_size = self.batch_size,\n",
    "                                         shuffle = False, drop_last = True)\n",
    "        return val_loader\n",
    "\n",
    "model = ConvNet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Logging & Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate Scheduling</td><td>▆▁▇▇▄▃▄▄▇▆▁▁▇▇▄▃▄█▇▂▁▆▆▅▄▃▃█▃▂▅▆▆▅▂▃██▃▂</td></tr><tr><td>class0_acc_train</td><td>▁▃▅▆▆▇▆▆▆▇▆▇▆▅▆▇▆▆▇▆▇▇▇▇▇█▇▇▇▇▇█▇▇▇▇▇███</td></tr><tr><td>class0_acc_val</td><td>███████████▇█▇▇█▇▇▇▇██▇▇▇▇▇▇▆▇▇▆▆▅▃▁▃▁▃▃</td></tr><tr><td>class1_acc_train</td><td>▁▄▄▄▅▅▆▆▆▆▇▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇█▇▇█▇█████▇███</td></tr><tr><td>class1_acc_val</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▅▄▅▅▄▅▄▅▄▅▆▅▆▅█▇▅▆▇▇▇▇▇███</td></tr><tr><td>class2_acc_train</td><td>▄▂▁▁▁▁▁▁▃▃▃▅▄▅▅▅▅▅▆▆▆▆▆▇▆▇▆▆▇▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>class2_acc_val</td><td>▂▁▁▁▂▂▃▄▅▄▆▆▅▆▆▇▆▇▆▆▆▇▇▇▇▇▇▆▇▇▇▇▇█▇▇█▆▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/acc</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val/acc</td><td>▁▁▂▁▂▁▃▃▄▄▅▆▅▆▇▆▆▇▆▆▆▇▆▇▇▇▇▇██▇▇█▇▇▆▇▆▇▇</td></tr><tr><td>val/loss</td><td>▇▇▇▇▇█▆▆▅▅▃▃▄▂▂▃▂▂▂▂▂▂▂▁▁▂▁▂▁▁▂▁▁▂▂▃▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate Scheduling</td><td>6e-05</td></tr><tr><td>class0_acc_train</td><td>0.88506</td></tr><tr><td>class0_acc_val</td><td>0.47619</td></tr><tr><td>class1_acc_train</td><td>0.80077</td></tr><tr><td>class1_acc_val</td><td>0.85714</td></tr><tr><td>class2_acc_train</td><td>0.68898</td></tr><tr><td>class2_acc_val</td><td>0.74138</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>train/acc</td><td>0.79253</td></tr><tr><td>train/loss</td><td>0.77883</td></tr><tr><td>trainer/global_step</td><td>19399</td></tr><tr><td>val/acc</td><td>0.69022</td></tr><tr><td>val/loss</td><td>1.17417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-butterfly-50</strong> at: <a href=\"https://wandb.ai/daydreamore/EEG_Analysis/runs/jess9gu5\" target=\"_blank\">https://wandb.ai/daydreamore/EEG_Analysis/runs/jess9gu5</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230309_144722-jess9gu5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230309_155520-a4kc7yki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/daydreamore/EEG_Analysis/runs/a4kc7yki\" target=\"_blank\">giddy-shadow-51</a></strong> to <a href=\"https://wandb.ai/daydreamore/EEG_Analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/daydreamore/EEG_Analysis\" target=\"_blank\">https://wandb.ai/daydreamore/EEG_Analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/daydreamore/EEG_Analysis/runs/a4kc7yki\" target=\"_blank\">https://wandb.ai/daydreamore/EEG_Analysis/runs/a4kc7yki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"EEG_Analysis\", log_model = True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath='C:/Users/Daydreamore/Desktop/Semester/BCI/model_checkpoints',\n",
    "#     monitor='val/acc',\n",
    "#     save_top_k=2\n",
    "# )\n",
    "    \n",
    "trainer = pl.Trainer(max_epochs = 200, gpus = 1, logger = wandb_logger,\n",
    "                    auto_lr_find = False, callbacks = [lr_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type               | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1       | Conv1d             | 14.3 K\n",
      "1  | conv2       | Conv1d             | 327 K \n",
      "2  | conv3       | Conv1d             | 163 K \n",
      "3  | conv4       | Conv1d             | 41.0 K\n",
      "4  | pool1       | AdaptiveMaxPool1d  | 0     \n",
      "5  | pool2       | AdaptiveMaxPool1d  | 0     \n",
      "6  | pool3       | AdaptiveMaxPool1d  | 0     \n",
      "7  | pool4       | AdaptiveMaxPool1d  | 0     \n",
      "8  | pool_final  | AdaptiveMaxPool1d  | 0     \n",
      "9  | lazy_linear | LazyLinear         | 0     \n",
      "10 | lazy_bn     | LazyBatchNorm1d    | 0     \n",
      "11 | lazy_bn2    | LazyBatchNorm1d    | 0     \n",
      "12 | lazy_bn3    | LazyBatchNorm1d    | 0     \n",
      "13 | GELU        | GELU               | 0     \n",
      "14 | dropout     | Dropout1d          | 0     \n",
      "15 | loss        | CrossEntropyLoss   | 0     \n",
      "16 | acc         | MulticlassAccuracy | 0     \n",
      "----------------------------------------------------\n",
      "547 K     Trainable params\n",
      "0         Non-trainable params\n",
      "547 K     Total params\n",
      "2.189     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 80/80 [00:03<00:00, 22.25it/s, loss=0.106, v_num=7yki, val/loss=0.285, val/acc=0.950, train/loss=0.139, train/acc=0.950] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 80/80 [00:03<00:00, 21.42it/s, loss=0.106, v_num=7yki, val/loss=0.285, val/acc=0.950, train/loss=0.139, train/acc=0.950]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
