{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import dataset as ds\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a dataset from the sub_preprocessed.npy files, where the samples for each condition are in separate folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dict_path, val_size = 0.2):\n",
    "    '''\n",
    "    dict_path: path with preprocessed npy files\n",
    "    val_size: rel. size of validations set\n",
    "    '''\n",
    "    if not os.path.exists(\"train_visual\"):\n",
    "        os.mkdir(\"train_visual\")\n",
    "        os.mkdir(\"val_visual\")\n",
    "        os.mkdir(\"train_multi\")\n",
    "        os.mkdir(\"val_multi\")\n",
    "    \n",
    "    experiments = {\n",
    "        \"visual\": [\"sub1_\", \"sub2\", \"sub3\", \"sub4\", \"sub5\", \"sub6\", \"sub7\", \"sub8\"],\n",
    "        \"multi\": [\"sub9\", \"sub10\", \"sub11\", \"sub12\", \"sub13\", \"sub14\", \"sub15\", \"sub16\"]\n",
    "    }\n",
    "    \n",
    "    id, t_count, v_count = 0, 0, 0\n",
    "    for experiment in experiments.keys():\n",
    "        train_labels, val_labels = [], []\n",
    "        for sub in experiments[experiment]:\n",
    "            id+=1\n",
    "            for path in natsorted(os.listdir(\"C:/Users/Daydreamore/Desktop/Semester/BCI\")):\n",
    "                if path.startswith(sub) & path.endswith(\"preprocessed.npy\"):\n",
    "                    sub_data = np.load(path, allow_pickle=True).item()\n",
    "                    for ix, condition in enumerate(sub_data.keys()):\n",
    "                        random.shuffle(sub_data[condition])\n",
    "                        split_ix = int(len(sub_data[condition])*val_size)\n",
    "                        train_set = sub_data[condition][split_ix:]\n",
    "                        val_set = sub_data[condition][:split_ix]\n",
    "                        train_labels.append([ix]*len(train_set))\n",
    "                        val_labels.append([ix]*len(val_set))\n",
    "                        \n",
    "                        for sample in train_set:\n",
    "                            np.save(\"train_{}/{}_sub{}_class{}.npy\".format(experiment, t_count, id, ix), sample[:,:165])\n",
    "                            t_count += 1\n",
    "\n",
    "                        for sample in val_set:\n",
    "                            np.save(\"val_{}/{}_sub{}_class{}.npy\".format(experiment, v_count, id, ix), sample[:,:165])\n",
    "                            v_count += 1\n",
    "                                \n",
    "        np.save(\"train_{}/labels.npy\".format(experiment), np.hstack(np.array(train_labels)))\n",
    "        np.save(\"val_{}/labels.npy\".format(experiment), np.hstack(np.array(val_labels)))\n",
    "\n",
    "#Create dataset\n",
    "make_dataset(\"C:/Users/Daydreamore/Desktop/Semester/BCI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Dataset-Class that can be indexed by Pytorch DataLoader.\n",
    "To save some GPU resources, we only pass the path such that the DataLoader (generator) loads one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class without undersampling\n",
    "class eeg_dataset():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.sample_list = os.listdir(path)[:-1]\n",
    "        self.targets = torch.from_numpy(np.load(os.path.join(path,os.listdir(path)[-1])))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (eeg_data, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        sample = np.load(os.path.join(self.path, self.sample_list[idx]))\n",
    "        # mean = np.mean(sample)\n",
    "        # std = np.std(sample)\n",
    "        # sample = (sample-mean)/std\n",
    "        return torch.from_numpy(sample), nn.functional.one_hot(self.targets[idx].to(torch.int64), num_classes = 3).float()\n",
    "\n",
    "path_train_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_visual\"\n",
    "path_train_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_multi\"\n",
    "path_val_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_visual\"\n",
    "path_val_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_multi\"\n",
    "train_set_visual = eeg_dataset(path_train_visual)\n",
    "train_set_multi = eeg_dataset(path_train_multi)\n",
    "val_set_visual = eeg_dataset(path_val_visual)\n",
    "val_set_multi = eeg_dataset(path_val_multi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if values in the sample can be accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_multi.__getitem__(1)[0][0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a highly unblanaced dataset. Therefore, we calculate a weight parameter to give a higher penalty to missprediction of less frequent class occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.from_numpy(train_set_multi.__len__() / (2 * np.bincount(train_set_multi.targets))).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 1D-ConvNet Setup (this could be further modularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #dataset = dataset_visual_under,\n",
    "        train_set = train_set_multi,\n",
    "        val_set = val_set_multi,\n",
    "        batch_size = 16,\n",
    "        epochs = 100,\n",
    "        learning_rate = 0.00003,\n",
    "        in_channels = 11,\n",
    "        out_channels = 256,\n",
    "        kernel_size = 5,\n",
    "        num_classes = 3,\n",
    "        bn_alpha = 0.1,\n",
    "        pool_out1 = 120,\n",
    "        pool_out2 = 60,\n",
    "        pool_out3 = 30,\n",
    "        pool_out4 = 15,\n",
    "        class_weights = class_weights,\n",
    "        dropout = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #Model Architecture Stuff\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size) \n",
    "        self.conv3 = nn.Conv1d(out_channels, 128, kernel_size) \n",
    "        self.conv4 = nn.Conv1d(128, 64, kernel_size)\n",
    "\n",
    "        #self.pool = nn.MaxPool1d(kernel_size=pool_kernel) #stride = kernel_size\n",
    "        self.pool1 = nn.AdaptiveMaxPool1d(pool_out1)\n",
    "        self.pool2 = nn.AdaptiveMaxPool1d(pool_out2)\n",
    "        self.pool3 = nn.AdaptiveMaxPool1d(pool_out3)\n",
    "        self.pool4 = nn.AdaptiveMaxPool1d(pool_out4)\n",
    "        self.pool_final = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.lazy_linear = nn.LazyLinear(out_features = num_classes)\n",
    "        self.lazy_bn = nn.LazyBatchNorm1d()\n",
    "        self.lazy_bn2 = nn.LazyBatchNorm1d()\n",
    "        self.lazy_bn3 = nn.LazyBatchNorm1d()\n",
    "        self.GELU = nn.GELU()\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.initialize_weights()\n",
    "\n",
    "        #Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        #self.dataset = dataset\n",
    "        #self.train_sampler = train_sampler\n",
    "        self.stepsize = np.around(self.train_set.__len__()*0.8/self.batch_size) #for cycling lr\n",
    "        #self.val_sampler = val_sampler\n",
    "        self.class_weights = class_weights #torch.from_numpy(train_set.__len__() / (2 * np.bincount(train_set.targets)))\n",
    "        self.loss = nn.CrossEntropyLoss(weight = self.class_weights) \n",
    "        self.acc = Accuracy(task = \"multiclass\", num_classes = num_classes)\n",
    "\n",
    "        #Log Hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #[32, 256, 166]\n",
    "        x = self.lazy_bn(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool1(x) #[32, 256, 120]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x) #[32, 256, 114]\n",
    "        x = self.lazy_bn(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool2(x) #[32, 256, 60]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x) #[32, 128, 56]\n",
    "        x = self.lazy_bn2(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool3(x) #[32, 128, 30]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x) #[32, 64, 26]\n",
    "        x = self.lazy_bn3(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.pool4(x) #[32, 64, 15]\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool_final(x) #[32, 64, 1]\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.lazy_linear(x)\n",
    "        x = self.GELU(x) #[32 x 3]\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(params = self.parameters(), lr = self.learning_rate, weight_decay = 0.01) #wd = 0.01 by default #lr_before = 0.00043\n",
    "        #parameters for the cycling lr scheduler are chosen according to Smith (2015): https://arxiv.org/pdf/1506.01186.pdf\n",
    "        lr = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimizer, base_lr = self.learning_rate,\n",
    "            max_lr = 4*self.learning_rate,\n",
    "            step_size_up = 4*int(self.stepsize),\n",
    "            mode = \"triangular\",\n",
    "            cycle_momentum = False\n",
    "            )\n",
    "        #Fix pickling bug for cycling learning rate (https://github.com/pytorch/pytorch/issues/88684)\n",
    "        #instantiate the WeakMethod in the lr scheduler object into the custom scale function attribute\n",
    "        lr._scale_fn_custom = lr._scale_fn_ref()\n",
    "        #remove the reference so there are no more WeakMethod references in the object\n",
    "        lr._scale_fn_ref = None\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": lr,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"Learning Rate Scheduling\"\n",
    "        }\n",
    "        # return {\"optimizer\": optimizer,\n",
    "        #         \"lr_scheduler\": lr_scheduler}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logit = self.forward(x.float())\n",
    "        train_loss = self.loss(logit, y)\n",
    "        _, y_pred = torch.max(logit, dim = 1)\n",
    "        _, y_true = torch.max(y, dim = 1)\n",
    "        return {\"loss\": train_loss, \"y_pred\": y_pred, \"y_true\": y_true}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss_epoch = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        y_pred_epoch = torch.stack([x[\"y_pred\"] for x in outputs]).flatten()\n",
    "        y_true_epoch = torch.stack([x[\"y_true\"] for x in outputs]).flatten()\n",
    "        class_acc = torch.bincount(y_true_epoch[y_true_epoch == y_pred_epoch])\n",
    "        class_count = torch.bincount(y_true_epoch)\n",
    "\n",
    "        #Log rel. amount of falsely predicted targets per class\n",
    "        for ix, tclass in enumerate(class_acc):\n",
    "            if tclass == 0:\n",
    "                self.log(f\"class{ix}_acc_train\", 0.0, on_epoch = True, prog_bar = False)\n",
    "            else:\n",
    "                self.log(f\"class{ix}_acc_train\", tclass/class_count[ix], on_epoch = True, prog_bar = False)\n",
    "        \n",
    "        train_acc = self.acc(y_pred_epoch, y_true_epoch)\n",
    "        self.log(\"train/loss\", train_loss_epoch, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"train/acc\", train_acc, on_epoch = True, prog_bar = True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logit = self.forward(x.float())\n",
    "        val_loss = self.loss(logit, y)\n",
    "        _, y_pred = torch.max(logit, dim = 1)\n",
    "        _, y_true = torch.max(y, dim = 1)\n",
    "        return {\"loss\": val_loss, \"y_pred\": y_pred, \"y_true\": y_true}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss_epoch = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        y_pred_epoch = torch.stack([x[\"y_pred\"] for x in outputs]).flatten()\n",
    "        y_true_epoch = torch.stack([x[\"y_true\"] for x in outputs]).flatten()\n",
    "        class_acc = torch.bincount(y_true_epoch[y_true_epoch == y_pred_epoch])\n",
    "        class_count = torch.bincount(y_true_epoch)\n",
    "\n",
    "        #Log rel. amount of falsely predicted targets per class\n",
    "        for ix, tclass in enumerate(class_acc):\n",
    "            if tclass == 0:\n",
    "                self.log(f\"class{ix}_acc_val\", 0.0, on_epoch = True, prog_bar = False)\n",
    "            else:\n",
    "                self.log(f\"class{ix}_acc_val\", tclass/class_count[ix], on_epoch = True, prog_bar = False)\n",
    "                \n",
    "        val_acc = self.acc(y_pred_epoch, y_true_epoch)\n",
    "        self.log(\"val/loss\", val_loss_epoch, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"val/acc\", val_acc, on_epoch = True, prog_bar = True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(self.train_set, batch_size = self.batch_size,\n",
    "                                           shuffle = True, drop_last = True)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(self.val_set, batch_size = self.batch_size,\n",
    "                                         shuffle = False, drop_last = True)\n",
    "        return val_loader\n",
    "\n",
    "model = ConvNet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Logging & Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"EEG_Analysis\", log_model = True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath='C:/Users/Daydreamore/Desktop/Semester/BCI/model_checkpoints',\n",
    "#     monitor='val/acc',\n",
    "#     save_top_k=2\n",
    "# )\n",
    "    \n",
    "trainer = pl.Trainer(max_epochs = 200, gpus = 1, logger = wandb_logger,\n",
    "                    auto_lr_find = False, callbacks = [lr_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
