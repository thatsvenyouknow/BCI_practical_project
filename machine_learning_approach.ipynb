{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "from scipy.signal import welch\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try individual features and see which one fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data_sets, target_dir):\n",
    "    \"\"\"\n",
    "    Function that does the feature engineering and saves the data into according folders.\n",
    "\n",
    "    Input:\n",
    "    - data_sets (list): datasets e.g. val_multi & train_multi\n",
    "    - target_dir: directory where data will be saved\n",
    "    \"\"\"\n",
    "    labels = None\n",
    "\n",
    "    for data_set in data_sets:\n",
    "        #load subject_data one by one\n",
    "        if not os.path.exists(target_dir+\"/var\"): #check if already preprocessed, else create directories\n",
    "            os.mkdir(target_dir+\"/var\")\n",
    "            os.mkdir(target_dir+\"/psd\")\n",
    "            os.mkdir(target_dir+\"/dwt\")\n",
    "            os.mkdir(target_dir+\"/var_psd_dwt\")\n",
    "\n",
    "        for path in natsorted(os.listdir(data_set))[:-1]:\n",
    "            file = np.load(os.path.join(data_set,path), allow_pickle=True)\n",
    "\n",
    "            #VAR\n",
    "            np_var = partial(np.var, axis=1)\n",
    "            a, b, c = np.split(file, [55, 110], axis=1)\n",
    "            VAR = np.hstack((\n",
    "                np_var(file),\n",
    "                np_var(a),\n",
    "                np_var(b),\n",
    "                np_var(c))\n",
    "                )\n",
    "            np.save(\"{}/{}\".format(target_dir+\"/var\", os.path.basename(path)), VAR)\n",
    "            #VAR = (VAR-np.mean(VAR))/np.std(VAR) #normalize\n",
    "\n",
    "            #PSD (Welch)\n",
    "            freq, power = welch(file, fs = 250)\n",
    "            PSD = power[:,:25].flatten() #take first 25 components (~35Hz)\n",
    "            np.save(\"{}/{}\".format(target_dir+\"/psd\", os.path.basename(path)), PSD)\n",
    "            #PSD = (PSD-np.mean(PSD))/np.std(PSD) #normalize\n",
    "\n",
    "            #DWT (only use approximate coefficients)\n",
    "            (aC, dC1, dC2, dC3) = pywt.wavedec(file, wavelet = \"db8\", mode='sym', level=3)\n",
    "            DWT = aC.flatten()\n",
    "            np.save(\"{}/{}\".format(target_dir+\"/dwt\", os.path.basename(path)), DWT)\n",
    "            #DWT = (DWT-np.mean(DWT))/np.std(DWT) #normalize\n",
    "\n",
    "            sample = np.hstack((VAR, PSD, DWT))\n",
    "            np.save(\"{}/{}\".format(target_dir+\"/var_psd_dwt\", os.path.basename(path)), sample)\n",
    "        \n",
    "        if labels is not None: #if labels have been loaded for first dataset, hstack and save for each method\n",
    "            labels = np.hstack((labels, np.load(data_set+\"/labels.npy\")))\n",
    "            np.save(\"{}/labels.npy\".format(target_dir), labels)\n",
    "        else: #for first dataset, labels are loaded into labels variable\n",
    "            labels = np.load(data_set+\"/labels.npy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:1999: UserWarning: nperseg = 256 is greater than input length  = 165, using nperseg = 165\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "#for audio-visual\n",
    "# train_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_multi\"\n",
    "# val_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_multi\"\n",
    "# data_ml_multi = \"C:/Users/Daydreamore/Desktop/Semester/BCI/data_ml_multi\"\n",
    "# feature_engineering([val_multi, train_multi], data_ml_multi)\n",
    "\n",
    "#for visual-only\n",
    "train_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/train_visual\"\n",
    "val_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/val_visual\"\n",
    "data_ml_visual = \"C:/Users/Daydreamore/Desktop/Semester/BCI/data_ml_visual\"\n",
    "feature_engineering([val_visual, train_visual], data_ml_visual)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets. \n",
    "Randomly select equal number of control trials for each participant to avoid unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, method, classes):\n",
    "    \"\"\"\n",
    "    Takes care of loading data, undersampling, and label concatenation\n",
    "\n",
    "    Input:\n",
    "    - data_path: path from which to load data (e.g. data_ml_multi)\n",
    "    - method: \"var\", \"psd\", \"dwt\", or \"var_psd_dwt\"\n",
    "    - classes (list): classes to load (e.g. [0,1] or [0,2])\n",
    "    \"\"\"\n",
    "    X_train = np.array([np.load(\"{}/{}/{}\".format(data_path, method, path)) for path in natsorted(os.listdir(data_path+\"/\"+method))[:]])\n",
    "    Y_train = np.load(\"{}/labels.npy\".format(data_path))\n",
    "    n_samp = np.where(Y_train == classes[1])[0].shape[0]\n",
    "\n",
    "    #For control randomly sample n_samp trials (to avoid imbalanced dataset)\n",
    "    for i in classes:\n",
    "        target_ixs = np.where(Y_train == i)\n",
    "\n",
    "        if i == 0: \n",
    "            target_ixs_shuffled = np.random.choice(target_ixs[0], size = n_samp, replace = False)\n",
    "            selected_samples = X_train[target_ixs_shuffled]\n",
    "            selected_labels = Y_train[target_ixs_shuffled]\n",
    "            X_train_selected = selected_samples\n",
    "            Y_train_selected = selected_labels\n",
    "        else:\n",
    "            selected_samples = X_train[target_ixs]\n",
    "            selected_labels = np.ones_like(Y_train[target_ixs])\n",
    "            X_train_selected = np.concatenate((X_train_selected, selected_samples))\n",
    "            Y_train_selected = np.concatenate((Y_train_selected, selected_labels))\n",
    "    return X_train_selected, Y_train_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vary the following to load the data for each method (var, psd, dwt, or combination) and for each class combination\n",
    "data_path = \"C:/Users/Daydreamore/Desktop/Semester/BCI/data_ml_multi\"\n",
    "method = \"var_psd_dwt\"\n",
    "classes = [0,1]\n",
    "X_train, Y_train = load_data(data_path, method, classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check to see if dimensions are as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((650, 682), (650,))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "Try to overfit on training data to see if the SVM can learn to tell the conditions apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8815384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "overfit = SVC(kernel = \"linear\")\n",
    "overfit.fit(X_train, Y_train)\n",
    "yhat = overfit.predict(X_train)\n",
    "acc_svm = accuracy_score(Y_train, yhat)\n",
    "print(acc_svm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine hyperparameter search with model selection through nested cross-validation and put out winning combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy=0.915, est=0.931, cfg={'C': 3.9576402424652394e-05, 'gamma': 3.814697265625e-06, 'kernel': 'linear'}\n",
      ">specificity: 0.943\n",
      ">sensitivity: 0.883\n",
      ">Accuracy=0.923, est=0.929, cfg={'C': 0.0004105939527606028, 'gamma': 3.814697265625e-06, 'kernel': 'linear'}\n",
      ">specificity: 0.917\n",
      ">sensitivity: 0.931\n",
      ">Accuracy=0.908, est=0.917, cfg={'C': 3.9576402424652394e-05, 'gamma': 3.814697265625e-06, 'kernel': 'linear'}\n",
      ">specificity: 0.967\n",
      ">sensitivity: 0.855\n",
      ">Accuracy=0.877, est=0.925, cfg={'C': 49.350746413054104, 'gamma': 3.814697265625e-06, 'kernel': 'rbf'}\n",
      ">specificity: 0.862\n",
      ">sensitivity: 0.889\n",
      ">Accuracy=0.954, est=0.929, cfg={'C': 0.004259795830723663, 'gamma': 3.814697265625e-06, 'kernel': 'linear'}\n",
      ">specificity: 0.969\n",
      ">sensitivity: 0.939\n",
      "----------Final Result----------\n",
      "Accuracy: 0.915 (0.025)\n",
      "Specificity: 0.932 (0.040)\n",
      "Sensitivity: 0.899 (0.031)\n"
     ]
    }
   ],
   "source": [
    "outer_results = list()\n",
    "specificity_results = list()\n",
    "sensitivity_results = list()\n",
    "#Outer CV\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_ix, val_ix in cv_outer.split(X_train): \n",
    "    #split data\n",
    "    x_train, x_val = X_train[train_ix, :], X_train[val_ix, :]\n",
    "    y_train, y_val = Y_train[train_ix], Y_train[val_ix]\n",
    "\n",
    "    #Inner CV\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    model = SVC()\n",
    "    #define search space\n",
    "    param_grid = dict()\n",
    "    param_grid[\"C\"] = np.logspace(-18, 9, num=9, base=2).tolist() \n",
    "    param_grid[\"kernel\"] = [\"linear\", \"sigmoid\", \"rbf\"] \n",
    "    param_grid[\"gamma\"] = np.logspace(-18, 9, num=9, base=2).tolist() \n",
    "    search = GridSearchCV(model, param_grid, scoring=\"accuracy\", cv=cv_inner, refit=True, error_score=\"raise\")\n",
    "    result = search.fit(x_train, y_train) #runs fit with all sets of parameters\n",
    "    best_model = result.best_estimator_ #get the best performing model fit on the whole training set\n",
    "    yhat = best_model.predict(x_val) #evaluate model on the hold out dataset\n",
    "    acc_svm = accuracy_score(y_val, yhat) #evaluate the model\n",
    "    outer_results.append(acc_svm) #store result\n",
    "    print(\">Accuracy=%.3f, est=%.3f, cfg=%s\" % (acc_svm, result.best_score_, result.best_params_)) #overall acc and best params\n",
    "\n",
    "    #sensitivity & specificity\n",
    "    class_acc = np.bincount(y_val[y_val == yhat])\n",
    "    class_count = np.bincount(y_val)\n",
    "\n",
    "    for ix, tclass in enumerate(class_acc):\n",
    "        if ix == 0:\n",
    "            condition = \"specificity\"\n",
    "        else:\n",
    "            condition = \"sensitivity\"\n",
    "\n",
    "        if tclass == 0:\n",
    "            if ix == 0:\n",
    "                specificity = 0.0\n",
    "                specificity_results.append(specificity)\n",
    "            else:\n",
    "                sensitivity = 0.0\n",
    "                sensitivity_results.append(sensitivity)\n",
    "            print(f\">{condition}: {0.0}\")\n",
    "        else:\n",
    "            if ix == 0:\n",
    "                specificity = np.around(tclass/class_count[ix], 3)\n",
    "                specificity_results.append(specificity)\n",
    "                print(f\">{condition}: {specificity}\")\n",
    "            else:\n",
    "                sensitivity = np.around(tclass/class_count[ix], 3)\n",
    "                sensitivity_results.append(sensitivity)\n",
    "                print(f\">{condition}: {sensitivity}\")\n",
    "\n",
    "            \n",
    "#summarize estimated performance of the model\n",
    "outer_results = np.array(outer_results)\n",
    "specificity_results = np.array(specificity_results)\n",
    "sensitivity_results = np.array(sensitivity_results)\n",
    "print(\"----------Final Result----------\")\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (np.mean(outer_results), np.std(outer_results)))\n",
    "print(\"Specificity: %.3f (%.3f)\" % (np.mean(specificity_results), np.std(specificity_results)))\n",
    "print(\"Sensitivity: %.3f (%.3f)\" % (np.mean(sensitivity_results), np.std(sensitivity_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
